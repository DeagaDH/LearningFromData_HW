{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# QUESTION 1\n",
    "\n",
    "Solving the given equation for N, we get to the form\n",
    "\n",
    "\\begin{equation}\n",
    "N = \\frac{(d+1)\\sigma^2}{\\sigma^2-E_{in}}\n",
    "\\end{equation}\n",
    "\n",
    "To find the $N$ that gives $E_{in} = 0.08$ with $\\sigma = 0.1$ and $d=8$, we can substitute these values, arriving at:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{N = 45}\n",
    "\\end{equation}\n",
    "\n",
    "Analysing the equation given by the problem, we can see that, as $N$ increases, so does the expected value of $E_in$. Therefore, the first alternative with $N > 45$ is the correct answer (\"which among the following choices is the smallest\n",
    "number of examples N that will result in an expected Ein greater than 0.008?\"). Therefore, the correct alternative is **alternative c (N_c = 100).**\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# QUESTION 2\n",
    "\n",
    "The hypothesis in the transformed space will be of form:\n",
    "\n",
    "\\begin{equation}\n",
    "h(s) = \\text{sign}(\\tilde{w}_0 + \\tilde{w}_1 x_1^2 + \\tilde{w}_2 x_2^2) \n",
    "\\end{equation}\n",
    "\n",
    "From the picture given in the problem, we know that the origin $x_1 = x_2 = 0$ has a +1 value, so that $h(s) = +1$ in that point. From this, it follows that $\\tilde{w}_0 = +1$. Next, we can look at values along the horizontal axis ($x_2 = 0$) where $x_1$ is large enough in either direction so that we end in the negative regions of the classification. In this situation, the hypothesis simplifies to:\n",
    "\n",
    "\\begin{equation}\n",
    "h(s) = \\text{sign}(\\tilde{w}_0 + \\tilde{w}_1 x_1^2) = -1\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\tilde{w}_0 = +1$ and $x_1 ^2 > 0$, we must have $\\tilde{w}_1 < 0$ for this equation to hold. Now consider a different point in the transformed space with the same $x_1$ coordinate, but a higher value of $x_2$ so that it $h(s)$ once again falls into the region classified as $+1$. In this case, we once again work with the complete hypothesis:\n",
    "\n",
    "\\begin{equation}\n",
    "h(s) = \\text{sign}(\\underbrace{\\tilde{w}_0}_{=+1} + \\underbrace{\\tilde{w}_1 x_1^2}_{< -1} + \\tilde{w}_2 x_2^2) = +1\n",
    "\\end{equation}\n",
    "\n",
    "From the previous discussion, we know that $\\tilde{w}_0 = +1$ and that not only $\\tilde{w}_1 < 0$, but also more specifically that $\\tilde{w}_1 x_1^2 < -1$, so that the point $(x_1,0)$ can fall into the negative region. If we are now taking a point $(x_1,x_2)$ with $x_2$ large enough to fall into the positive region, it folows that we must have $\\tilde{w}_2 > 0$, so that the above equation can hold.\n",
    "\n",
    "Therefore, it has been found that $\\tilde{w}_1 < 0$ and $\\tilde{w}_2 > 0$. The alternative that correctly describes this scenario is **alternative d**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# QUESTION 3\n",
    "\n",
    "From the $\\Phi$ function given, we know there are $14$ parameters in the model (not counting the fixed parameter 1), which gives us a VC dimension $d_{VC} = 14+1 = 15$.  Therefore, the smallest alternative that is not smaller than $d_{VC}$ is **alternative c (15).**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# QUESTION 4\n",
    "\n",
    "Applying the chain rule, the correct alternative is **alternative e**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# QUESTIONS 5 AND 6 (CODE)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\n(u,v) = (0.045,0.024)\n"
     ]
    }
   ],
   "source": [
    "from math import e\n",
    "\n",
    "#Implement functions for the error function and its two partial derivatives\n",
    "\n",
    "def E(u,v):\n",
    "    return (u*e**v - 2*v*e**-u)**2\n",
    "\n",
    "def dEdu(u,v):\n",
    "    return 2*(u*e**v - 2*v*e**-u)*(e**v + 2*v*e**-u)\n",
    "\n",
    "def dEdv(u,v):\n",
    "    return 2*(u*e**v - 2*v*e**-u)*(u*e**v - 2*e**-u)\n",
    "\n",
    "def gradE(u,v):\n",
    "    return dEdu(u,v) + dEdv(u,v)\n",
    "\n",
    "#Initialize u and v\n",
    "u=1\n",
    "v=1\n",
    "\n",
    "#Evaluate initial error\n",
    "error=E(u,v)\n",
    "\n",
    "#Iterate until error is smaller than a tolerance and count iterations\n",
    "tol=10**-14\n",
    "it=0\n",
    "\n",
    "#Learning rate\n",
    "eta=0.1\n",
    "\n",
    " \n",
    "while True:\n",
    "    #Save old error value\n",
    "    error_old = error\n",
    "    \n",
    "    #Increment iteration counter\n",
    "    it +=1\n",
    "\n",
    "    #Evalute new u and v from gradient descent -> a step in the gradient\n",
    "    u_new = u - eta*dEdu(u,v)\n",
    "    v -= eta*dEdv(u,v)\n",
    "    u = u_new #Only overwrite u later to properly evaluate v\n",
    "\n",
    "    #Calculate new error value\n",
    "    error = E(u,v)\n",
    "\n",
    "    if (error < tol or it > 1000):\n",
    "        break\n",
    "\n",
    "print(it)\n",
    "print(f'(u,v) = ({round(u,3)},{round(v,3)})')\n"
   ]
  },
  {
   "source": [
    "# QUESTIONS 5 AND 6 (ANSWERS)\n",
    "\n",
    "### Question 5\n",
    "From the above code, we can see the correct alternative is **alternative d (10 iterations)**.\n",
    "\n",
    "### Question 6\n",
    "From the above code, we can see the correct alternative is **alternative e ((u,v) = (0.045,0.024))**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# QUESTION 7 (CODE)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.13981379199615324\n(u,v) = (6.297,-2.852)\n"
     ]
    }
   ],
   "source": [
    "from math import e\n",
    "\n",
    "#Implement functions for the error function and its two partial derivatives\n",
    "\n",
    "def E(u,v):\n",
    "    return (u*e**v - 2*v*e**-u)**2\n",
    "\n",
    "def dEdu(u,v):\n",
    "    return 2*(u*e**v - 2*v*e**-u)*(e**v + 2*v*e**-u)\n",
    "\n",
    "def dEdv(u,v):\n",
    "    return 2*(u*e**v - 2*v*e**-u)*(u*e**v - 2*e**-u)\n",
    "\n",
    "def gradE(u,v):\n",
    "    return dEdu(u,v) + dEdv(u,v)\n",
    "\n",
    "#Initialize u and v\n",
    "u=1\n",
    "v=1\n",
    "\n",
    "#Evaluate initial error\n",
    "error=E(u,v)\n",
    "\n",
    "#Repeat for 'it' iterations\n",
    "it = 15\n",
    "\n",
    "#Learning rate\n",
    "eta=0.1\n",
    "\n",
    " \n",
    "for i in range(it):\n",
    "    #Save old error value\n",
    "    error_old = error\n",
    "    \n",
    "    #Increment iteration counter\n",
    "    it +=1\n",
    "\n",
    "    # Evalute new u from coordinate descent\n",
    "    u -= eta*dEdu(u,v)\n",
    "\n",
    "    # Now evaluate v\n",
    "    v -= eta*dEdv(u,v)\n",
    "\n",
    "    #Calculate new error value\n",
    "    error = E(u,v)\n",
    "\n",
    "\n",
    "print(error)\n",
    "print(f'(u,v) = ({round(u,3)},{round(v,3)})')\n"
   ]
  },
  {
   "source": [
    "# QUESTION 7\n",
    "\n",
    "From the above code, we can see that the error term, after 15 iterations, has order of $10^{-1}$. Therefore, the correct alternative is **alternative a.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# QUESTIONS 8 AND 9 (CODE)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running experiment number 1...\n",
      "Running experiment number 10...\n",
      "Running experiment number 20...\n",
      "Running experiment number 30...\n",
      "Running experiment number 40...\n",
      "Running experiment number 50...\n",
      "Running experiment number 60...\n",
      "Running experiment number 70...\n",
      "Running experiment number 80...\n",
      "Running experiment number 90...\n",
      "Running experiment number 100...\n",
      "\n",
      "Average number of iterations to converge: 349\n",
      "Average error out of sample: 0.1\n"
     ]
    }
   ],
   "source": [
    "from hw5_func_array import *\n",
    "\n",
    "# How many points to use?\n",
    "N = 100\n",
    "\n",
    "# How many experiments to run?\n",
    "N_exp=100\n",
    "\n",
    "# How many test points for Eout?\n",
    "N_test=N\n",
    "\n",
    "# Limit values of x and y\n",
    "xlim = [-1,1]\n",
    "ylim = [-1,1]\n",
    "\n",
    "#Maximum number of iterations to try the Logistic Regression algorithm\n",
    "max_iter=1000\n",
    "\n",
    "#Create variable to store average number of iterations per experiment\n",
    "avg_iter=0\n",
    "\n",
    "#Variable to store error probability\n",
    "error=0\n",
    "\n",
    "for exp in range(0,N_exp):\n",
    "\n",
    "    #Display current experiment in console, every 50 experiments\n",
    "    if (exp+1 == 1) or ((exp+1) % 10 ==0):\n",
    "        print(f'Running experiment number {exp+1}...')\n",
    "\n",
    "    #Target line\n",
    "    target_line = line(random=True,xlim=xlim,ylim=ylim)\n",
    "    target_function = target_line.map   \n",
    "\n",
    "    #Initialize a data set of x (point coordinates) and y (target values)\n",
    "    x,y = create_dataset(random_point,target_function,N)\n",
    "\n",
    "    #Initialize perceptron with the x and y lists\n",
    "    lr = LogisticRegression(x,y)\n",
    "\n",
    "    #Apply the learning algorithm and store iteration count\n",
    "    avg_iter += lr.learn(max_iter=max_iter,tol=0.01)\n",
    "\n",
    "    #Now create a test dataset\n",
    "    x_test,y_test = create_dataset(random_point,target_function,N_test)\n",
    "\n",
    "    #Test learning\n",
    "    error += lr.test_learning(x_test,y_test)\n",
    "\n",
    "#Print results\n",
    "print(f'\\nAverage number of iterations to converge: {int(round(avg_iter/N_exp,0))}')\n",
    "print(f'Average error out of sample: {(round(error/N_exp,2))}')\n"
   ]
  },
  {
   "source": [
    "# QUESTIONS 8 AND 9 (ANSWERS)\n",
    "\n",
    "### Question 8\n",
    "From the above code, we can see the correct alternative is **alternative d ($E_{out} = 0.1$)**.\n",
    "\n",
    "### Question 9\n",
    "From the above code, we can see the correct alternative is **alternative a (340 iterations)**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# QUESTION 10\n",
    "\n",
    "The essence of the Perceptron Learning Algorithm (PLA) is that it will try to correct the weights only on misclassified points. Additionally, the algorithm works on binary classification (ie +1 or -1) rather than a probability, which is what allows us to cleanly separate points in \"correct\" or \"incorrect\". The function $h(s)$ for the PLA was:\n",
    "\n",
    "\\begin{equation}\n",
    "h(s) = \\text{sign}(\\boldsymbol w^T \\boldsymbol x)\n",
    "\\end{equation}\n",
    "\n",
    "Note that $h(s) = {-1,+1}$, and the target values are also $y_n = {-1,+1}$. This gives us an interesting property, which we can use to define a possible error function shown below:\n",
    "\n",
    "\\begin{equation}\n",
    "e(\\boldsymbol w) = y h(s) = y \\text{sign}(\\boldsymbol w^T \\boldsymbol x)\n",
    "\\end{equation}\n",
    "\n",
    "Note that $e(\\boldsymbol w) = +1$ if, and only if, $y = h(s)$ and, similarly, $e(\\boldsymbol w) = -1$ if and only if $y \\neq h(s)$. Therefore, we can detect if a point is misclassified using this form of the error function. However, this form is also not differentiable due to the sign function. A simpler form, that is still able to detect misclassiffied points, is shown below:\n",
    "\n",
    "\\begin{equation}\n",
    "e(\\boldsymbol w) = y \\boldsymbol w^T \\boldsymbol x\n",
    "\\end{equation}\n",
    "\n",
    "For this form of the error function, we have $e(\\boldsymbol w) > 0$ if, and only if, $\\text{sign}(w^T \\boldsymbol x) = y$ (ie properly classified) and conversely, $e(\\boldsymbol w) < 0$ if, and only if, $\\text{sign}(w^T \\boldsymbol x) \\neq y$ (ie misclassified). For the PLA, the weights shall only be modified for incorrect points. Therefore, we can remove the correct points from the above equation by choosing the minimum value between itself and zero, as shown below:\n",
    "\n",
    "\\begin{equation}\n",
    "e(\\boldsymbol w) = - min(0,y \\boldsymbol w^T \\boldsymbol x)\n",
    "\\end{equation}\n",
    "\n",
    "This form of $e(\\boldsymbol w)$ will be exactly 0 on properly classified points. The added negative sign ensures that, whe the function is not 0, it will have negative values. Therefore, minimizing the error function will be a fruitful endeavor.\n",
    "\n",
    "The alternative that displays the error function found here is **alternative e**.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}